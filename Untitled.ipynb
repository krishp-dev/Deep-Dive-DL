{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "955aca63-3300-4eac-86e2-daf8af975dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in c:\\users\\dell\\anaconda3\\envs\\py311_env\\lib\\site-packages (11.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "343dcf5e-22a3-4553-8e91-bac6b23aff01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up models...\n",
      "Models ready!\n",
      "\n",
      "Fruit/Vegetable Quality Detection System\n",
      "========================================\n",
      "1. Real-time detection from webcam\n",
      "2. Process single image\n",
      "3. Process video file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your choice (1-3):  3\n",
      "Enter video path:  hgh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not open video source\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nTO IMPROVE PERFORMANCE:\\n\\n1. Use actual YOLO weights:\\n   - Download YOLOv5 or YOLOv8 weights\\n   - Load with: model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\\n\\n2. GPU acceleration:\\n   - Use CUDA for PyTorch/TensorFlow models\\n   - Set device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n\\n3. Real training data:\\n   - Collect images of fresh/rotten fruits and vegetables\\n   - Label them as good/medium/bad quality\\n   - Train the CNN model on real data\\n\\n4. Advanced image processing:\\n   - Use more sophisticated color analysis\\n   - Implement texture analysis for surface defects\\n   - Add edge detection for shape irregularities\\n\\n5. Model optimization:\\n   - Use model quantization for faster inference\\n   - Implement TensorRT for NVIDIA GPUs\\n   - Use OpenVINO for Intel hardware\\n\\n6. Multi-threading:\\n   - Separate detection and quality analysis threads\\n   - Use queue system for frame processing\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class FruitVegetableQualityDetector:\n",
    "    def __init__(self):\n",
    "        self.yolo_model = None\n",
    "        self.quality_model = None\n",
    "        self.fruit_veggie_classes = [\n",
    "            'apple', 'banana', 'orange', 'strawberry', 'grape', 'pineapple',\n",
    "            'watermelon', 'mango', 'kiwi', 'peach', 'pear', 'cherry',\n",
    "            'carrot', 'broccoli', 'potato', 'tomato', 'cucumber', 'pepper',\n",
    "            'onion', 'lettuce', 'spinach', 'cabbage', 'corn', 'eggplant'\n",
    "        ]\n",
    "        self.setup_models()\n",
    "    \n",
    "    def setup_models(self):\n",
    "        \"\"\"Initialize YOLO and quality assessment models\"\"\"\n",
    "        # For demonstration, we'll create a simplified version\n",
    "        # In practice, you'd load pre-trained YOLO weights\n",
    "        print(\"Setting up models...\")\n",
    "        self.setup_quality_model()\n",
    "        print(\"Models ready!\")\n",
    "    \n",
    "    def setup_quality_model(self):\n",
    "        \"\"\"Create a CNN model for quality assessment\"\"\"\n",
    "        model = keras.Sequential([\n",
    "            layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(512, activation='relu'),\n",
    "            layers.Dense(3, activation='softmax')  # Good, Medium, Bad\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        self.quality_model = model\n",
    "    \n",
    "    def detect_objects_yolo(self, frame):\n",
    "        \"\"\"Simplified YOLO detection - in practice use actual YOLO model\"\"\"\n",
    "        # This is a simplified version for demonstration\n",
    "        # You would load actual YOLO weights and perform detection\n",
    "        \n",
    "        # Simulate detection results\n",
    "        height, width = frame.shape[:2]\n",
    "        \n",
    "        # Mock detections for demonstration\n",
    "        detections = [\n",
    "            {\n",
    "                'class': 'apple',\n",
    "                'confidence': 0.85,\n",
    "                'bbox': [width//4, height//4, width//2, height//2],\n",
    "                'center': [width//2, height//2]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return detections\n",
    "    \n",
    "    def zoom_and_crop(self, frame, bbox, zoom_factor=1.5):\n",
    "        \"\"\"Zoom into detected object\"\"\"\n",
    "        x, y, w, h = bbox\n",
    "        \n",
    "        # Calculate expanded bbox\n",
    "        center_x, center_y = x + w//2, y + h//2\n",
    "        new_w, new_h = int(w * zoom_factor), int(h * zoom_factor)\n",
    "        \n",
    "        # Ensure bounds\n",
    "        x1 = max(0, center_x - new_w//2)\n",
    "        y1 = max(0, center_y - new_h//2)\n",
    "        x2 = min(frame.shape[1], center_x + new_w//2)\n",
    "        y2 = min(frame.shape[0], center_y + new_h//2)\n",
    "        \n",
    "        cropped = frame[y1:y2, x1:x2]\n",
    "        \n",
    "        # Resize to standard size for analysis\n",
    "        if cropped.size > 0:\n",
    "            cropped = cv2.resize(cropped, (224, 224))\n",
    "        \n",
    "        return cropped, (x1, y1, x2, y2)\n",
    "    \n",
    "    def analyze_wastage_opencv(self, image):\n",
    "        \"\"\"Analyze wastage using OpenCV image processing\"\"\"\n",
    "        if image.size == 0:\n",
    "            return 0, 0, []\n",
    "        \n",
    "        # Convert to different color spaces\n",
    "        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "        \n",
    "        # Detect brown/dark spots (potential rot)\n",
    "        # HSV ranges for brown/dark colors\n",
    "        lower_brown = np.array([8, 50, 20])\n",
    "        upper_brown = np.array([20, 255, 150])\n",
    "        brown_mask = cv2.inRange(hsv, lower_brown, upper_brown)\n",
    "        \n",
    "        # Detect very dark areas\n",
    "        lower_dark = np.array([0, 0, 0])\n",
    "        upper_dark = np.array([180, 255, 50])\n",
    "        dark_mask = cv2.inRange(hsv, lower_dark, upper_dark)\n",
    "        \n",
    "        # Combine masks\n",
    "        damage_mask = cv2.bitwise_or(brown_mask, dark_mask)\n",
    "        \n",
    "        # Apply morphological operations to clean up\n",
    "        kernel = np.ones((5,5), np.uint8)\n",
    "        damage_mask = cv2.morphologyEx(damage_mask, cv2.MORPH_CLOSE, kernel)\n",
    "        damage_mask = cv2.morphologyEx(damage_mask, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        # Find contours of damaged areas\n",
    "        contours, _ = cv2.findContours(damage_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # Calculate areas\n",
    "        total_area = image.shape[0] * image.shape[1]\n",
    "        damage_area = cv2.countNonZero(damage_mask)\n",
    "        \n",
    "        # Calculate percentages\n",
    "        damage_percentage = (damage_area / total_area) * 100\n",
    "        good_percentage = 100 - damage_percentage\n",
    "        \n",
    "        return good_percentage, damage_percentage, contours\n",
    "    \n",
    "    def draw_labels_and_info(self, frame, bbox, class_name, confidence, good_pct, bad_pct, contours, zoomed_bbox):\n",
    "        \"\"\"Draw bounding boxes, labels, and quality information\"\"\"\n",
    "        x, y, w, h = bbox\n",
    "        \n",
    "        # Draw main bounding box\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw zoomed area box\n",
    "        if zoomed_bbox:\n",
    "            x1, y1, x2, y2 = zoomed_bbox\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "        \n",
    "        # Prepare label text\n",
    "        label = f\"{class_name}: {confidence:.2f}\"\n",
    "        quality_text = f\"Good: {good_pct:.1f}% | Bad: {bad_pct:.1f}%\"\n",
    "        \n",
    "        # Draw labels\n",
    "        label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]\n",
    "        cv2.rectangle(frame, (x, y - 30), (x + label_size[0], y), (0, 255, 0), -1)\n",
    "        cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
    "        \n",
    "        # Draw quality info\n",
    "        quality_size = cv2.getTextSize(quality_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n",
    "        cv2.rectangle(frame, (x, y + h), (x + quality_size[0], y + h + 25), (255, 255, 255), -1)\n",
    "        cv2.putText(frame, quality_text, (x, y + h + 18), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "        \n",
    "        # Color code based on quality\n",
    "        if good_pct > 80:\n",
    "            color = (0, 255, 0)  # Green for good\n",
    "        elif good_pct > 60:\n",
    "            color = (0, 255, 255)  # Yellow for medium\n",
    "        else:\n",
    "            color = (0, 0, 255)  # Red for bad\n",
    "        \n",
    "        # Draw quality indicator\n",
    "        cv2.circle(frame, (x + w - 20, y + 20), 10, color, -1)\n",
    "        \n",
    "        return frame\n",
    "    \n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"Process a single frame\"\"\"\n",
    "        # Detect objects using YOLO\n",
    "        detections = self.detect_objects_yolo(frame)\n",
    "        \n",
    "        processed_frame = frame.copy()\n",
    "        \n",
    "        for detection in detections:\n",
    "            class_name = detection['class']\n",
    "            confidence = detection['confidence']\n",
    "            bbox = detection['bbox']\n",
    "            \n",
    "            # Only process fruits and vegetables\n",
    "            if class_name.lower() in [c.lower() for c in self.fruit_veggie_classes]:\n",
    "                # Zoom and crop the detected object\n",
    "                cropped_img, zoomed_bbox = self.zoom_and_crop(frame, bbox)\n",
    "                \n",
    "                if cropped_img.size > 0:\n",
    "                    # Analyze wastage\n",
    "                    good_pct, bad_pct, contours = self.analyze_wastage_opencv(cropped_img)\n",
    "                    \n",
    "                    # Draw labels and information\n",
    "                    processed_frame = self.draw_labels_and_info(\n",
    "                        processed_frame, bbox, class_name, confidence, \n",
    "                        good_pct, bad_pct, contours, zoomed_bbox\n",
    "                    )\n",
    "        \n",
    "        return processed_frame\n",
    "    \n",
    "    def run_real_time_detection(self, source=0):\n",
    "        \"\"\"Run real-time detection from webcam or video file\"\"\"\n",
    "        cap = cv2.VideoCapture(source)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Could not open video source\")\n",
    "            return\n",
    "        \n",
    "        print(\"Starting real-time detection. Press 'q' to quit.\")\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Process frame\n",
    "            processed_frame = self.process_frame(frame)\n",
    "            \n",
    "            # Display result\n",
    "            cv2.imshow('Fruit/Vegetable Quality Detection', processed_frame)\n",
    "            \n",
    "            # Break on 'q' key press\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    def process_single_image(self, image_path):\n",
    "        \"\"\"Process a single image file\"\"\"\n",
    "        frame = cv2.imread(image_path)\n",
    "        if frame is None:\n",
    "            print(f\"Error: Could not load image {image_path}\")\n",
    "            return None\n",
    "        \n",
    "        processed_frame = self.process_frame(frame)\n",
    "        \n",
    "        # Display result\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Original Image')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Processed Image with Quality Analysis')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return processed_frame\n",
    "\n",
    "# Example usage and training data generator\n",
    "class QualityDataGenerator:\n",
    "    \"\"\"Helper class to generate training data for quality assessment\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_synthetic_training_data():\n",
    "        \"\"\"Create synthetic training data for the quality model\"\"\"\n",
    "        # This would typically load real images of good/medium/bad fruits\n",
    "        # For demonstration, we create random data\n",
    "        \n",
    "        X_train = np.random.rand(1000, 224, 224, 3)\n",
    "        y_train = np.random.randint(0, 3, (1000, 3))  # One-hot encoded\n",
    "        \n",
    "        return X_train, y_train\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_quality_model(detector):\n",
    "        \"\"\"Train the quality assessment model\"\"\"\n",
    "        print(\"Generating training data...\")\n",
    "        X_train, y_train = QualityDataGenerator.create_synthetic_training_data()\n",
    "        \n",
    "        print(\"Training quality model...\")\n",
    "        detector.quality_model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=5,\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the detector\n",
    "    detector = FruitVegetableQualityDetector()\n",
    "    \n",
    "    # Optional: Train the quality model with your own data\n",
    "    # data_generator = QualityDataGenerator()\n",
    "    # data_generator.train_quality_model(detector)\n",
    "    \n",
    "    print(\"\\nFruit/Vegetable Quality Detection System\")\n",
    "    print(\"========================================\")\n",
    "    print(\"1. Real-time detection from webcam\")\n",
    "    print(\"2. Process single image\")\n",
    "    print(\"3. Process video file\")\n",
    "    \n",
    "    choice = input(\"Enter your choice (1-3): \")\n",
    "    \n",
    "    if choice == '1':\n",
    "        detector.run_real_time_detection(0)  # Webcam\n",
    "    elif choice == '2':\n",
    "        image_path = input(\"Enter image path: \")\n",
    "        detector.process_single_image(image_path)\n",
    "    elif choice == '3':\n",
    "        video_path = input(\"Enter video path: \")\n",
    "        detector.run_real_time_detection(video_path)\n",
    "    else:\n",
    "        print(\"Invalid choice\")\n",
    "\n",
    "# Additional utility functions\n",
    "def save_model(detector, model_path):\n",
    "    \"\"\"Save the trained quality model\"\"\"\n",
    "    detector.quality_model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "def load_model(detector, model_path):\n",
    "    \"\"\"Load a pre-trained quality model\"\"\"\n",
    "    detector.quality_model = keras.models.load_model(model_path)\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "\n",
    "def batch_process_images(detector, image_folder, output_folder):\n",
    "    \"\"\"Process multiple images in batch\"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for filename in os.listdir(image_folder):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(image_folder, filename)\n",
    "            frame = cv2.imread(image_path)\n",
    "            \n",
    "            if frame is not None:\n",
    "                processed_frame = detector.process_frame(frame)\n",
    "                output_path = os.path.join(output_folder, f\"processed_{filename}\")\n",
    "                cv2.imwrite(output_path, processed_frame)\n",
    "                print(f\"Processed: {filename}\")\n",
    "\n",
    "# Performance optimization suggestions:\n",
    "\"\"\"\n",
    "TO IMPROVE PERFORMANCE:\n",
    "\n",
    "1. Use actual YOLO weights:\n",
    "   - Download YOLOv5 or YOLOv8 weights\n",
    "   - Load with: model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "\n",
    "2. GPU acceleration:\n",
    "   - Use CUDA for PyTorch/TensorFlow models\n",
    "   - Set device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "3. Real training data:\n",
    "   - Collect images of fresh/rotten fruits and vegetables\n",
    "   - Label them as good/medium/bad quality\n",
    "   - Train the CNN model on real data\n",
    "\n",
    "4. Advanced image processing:\n",
    "   - Use more sophisticated color analysis\n",
    "   - Implement texture analysis for surface defects\n",
    "   - Add edge detection for shape irregularities\n",
    "\n",
    "5. Model optimization:\n",
    "   - Use model quantization for faster inference\n",
    "   - Implement TensorRT for NVIDIA GPUs\n",
    "   - Use OpenVINO for Intel hardware\n",
    "\n",
    "6. Multi-threading:\n",
    "   - Separate detection and quality analysis threads\n",
    "   - Use queue system for frame processing\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34f9f5a-06b6-40b8-b7b7-7270ca44620a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
